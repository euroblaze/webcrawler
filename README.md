# ğŸ•¸ï¸ Crawl4AI FastAPI Service

This project exposes [`crawl4ai`](https://pypi.org/project/crawl4ai/) as a Dockerized microservice with a FastAPI interface for programmatically crawling web content and storing results as sanitized Markdown files. It supports crawl depth and timeout controls per request, persistent crawl status storage via SQLite, logging, and is ready for Celery integration.

---

## ğŸ›  Features

* FastAPI-based web service for managing crawls
* Persistent crawl ID and status tracking using SQLite
* Configurable and per-request crawl depth and timeout
* HTML sanitization using `bleach`
* Persistent logs written to `/data/crawl4ai.log`
* File-based crawl results (`/data/<crawl_id>.md`)
* Configurable via `crawl.config`
* Dockerized setup with `docker-compose`

---

## ğŸ“ Project Structure

```
crawl4ai-service/
â”œâ”€â”€ app/                 # FastAPI backend and logic
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ db.py
â”‚   â””â”€â”€ auth_dummy.py
â”œâ”€â”€ data/                # Output files + logs + sqlite db
â”œâ”€â”€ crawl.config         # Default settings (depth, timeout)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ docker-compose.yaml
```

---

## ğŸš€ Quickstart

### 1. Build & Run

```bash
docker-compose up --build
```

### 2. Test API (with curl)

```bash
curl -X POST http://localhost:8000/crawl \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com", "crawl_depth": 2, "crawl_timeout": 5}'
```

---

## ğŸ§© Configuration

Edit `crawl.config`:

```ini
[settings]
crawl_depth = 2        # Default max crawl depth
crawl_timeout = 10     # Default timeout per crawl (seconds)
```

---

## ğŸ“œ API Endpoints

| Endpoint             | Method | Description                        |
| -------------------- | ------ | ---------------------------------- |
| `/crawl`             | POST   | Trigger a new crawl                |
| `/status/{crawl_id}` | GET    | Check crawl status                 |
| `/result/{crawl_id}` | GET    | Retrieve sanitized Markdown result |
| `/list`              | GET    | List all stored `.md` results      |
| `/health`            | GET    | Simple health check                |

---

## ğŸ›¡ï¸ Security & Improvements

* [ ] Add authentication (OAuth2 or API Key)
* [ ] Replace file-based crawl ID with atomic SQLite sequence
* [ ] Add CORS headers for browser clients
* [ ] Integrate Celery for background queueing (SQLite â†’ Redis)
* [ ] Implement file size limits / storage cleanup policies

---

## ğŸ“ License

MIT License.
